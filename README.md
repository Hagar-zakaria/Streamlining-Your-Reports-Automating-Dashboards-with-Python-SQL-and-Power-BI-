# Streamlining-Your-Reports-Automating-Dashboards-with-Python-SQL-and-Power-BI-
As a data analyst, this is one of the most interesting projects you'll work on because you'll see how business metrics change in real time. In the real world, companies receive new data daily, so dynamic dashboard reports are essential.

# WHAT ARE DYNAMIC DASHBARD REPORTS?
Dynamic dashboard reports are essentially real-time snapshots of data that reflect the latest updates or changes in a database.

For example, let's consider an online store. Yesterday, it received 2500 orders, but today it received 2800 orders. A static dashboard would only display the total number of orders from the previous day (2500). However, a dynamic dashboard report would show the most recent data, indicating that the store received 2800 orders today. It can also illustrate the growth in the number of orders between the previous day and the current day.

Imagine you're viewing such a dashboard in your business intelligence tool. Once you refresh the dashboard page, the dynamic report immediately reflects the most recent changes in the data. This capability ensures that users always have access to the latest information, allowing for better-informed decisions and insights.

![image](https://github.com/Hagar-zakaria/Streamlining-Your-Reports-Automating-Dashboards-with-Python-SQL-and-Power-BI-/assets/93611934/1ab5e3d0-466f-42f4-88ff-d0a4a5abdbf9)

Dynamic dashboard reports are crucial for tracking key business growth metrics such as customer churn/retention rate, revenue growth/decline, and new customer count on various time scales like daily, weekly, monthly, or yearly.

In this project, I'll be creating a dynamic dashboard report using data from a cryptocurrency web API. This API offers data on the revenue generated by multiple cryptocurrency projects each day.

The project aims to analyze revenue growth over time, daily revenue, monitor the active cryptocurrency projects generating revenue daily, and track the 24-hour revenue change.

Here's an example of the report we'll generate daily.

![image](https://github.com/Hagar-zakaria/Streamlining-Your-Reports-Automating-Dashboards-with-Python-SQL-and-Power-BI-/assets/93611934/6e2fe7a5-d2f0-425b-95d4-996e14395f89)

# Tools and tech used:

1. Python
2. SQL SERVER DATABASE
3. POWER BI
4. Windows task scheduler

# PROJECT ARCHITECTURE

![image](https://github.com/Hagar-zakaria/Streamlining-Your-Reports-Automating-Dashboards-with-Python-SQL-and-Power-BI-/assets/93611934/cc12ef96-f55a-4202-a85b-ee61a2d00405)

# STEP 1

Dynamic dashboard reports are crucial for tracking key business growth metrics such as customer churn/retention rate, revenue growth/decline, and new customer count on various time scales like daily, weekly, monthly, or yearly.

In this project, I'll be creating a dynamic dashboard report using data from a cryptocurrency web API. This API offers data on the revenue generated by multiple cryptocurrency projects each day.

The project aims to analyze revenue growth over time, daily revenue, monitor the active cryptocurrency projects generating revenue daily, and track the 24-hour revenue change.

Here's an example of the report we'll generate daily.

'''# Importing needed libraries
import pandas as pd
from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Numeric
import os
import requests
from datetime import date
import sqlalchemy
import pyodbc
           '''

# Getting current date because the data doesn't come with a date column
now = date.today()
todays_date = now.strftime('%Y/%m/%d')

# Extracting data from Ethereum API
eth_api = 'https://api.llama.fi/overview/fees/ethereum?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue'
params = {'chain':'Ethereum'}
r = requests.get(eth_api, params=params)
eth_json = r.json()
eth_df = pd.DataFrame(eth_json['protocols'])

# Creating a function to extract data from any API link provided
def extract_from_api(api_url, chain_name, params):
    params = {'chain': chain_name}
    r = requests.get(api_url, params=params)
    chain_json = r.json()
    chain_df = pd.DataFrame(chain_json['protocols'])
    return chain_df

# This function transforms the data, and keeps the necessary columns needed
def transform_data(chain_df, chain_name):
    cols = ['defillamaId', 'name', 'module', 'category', 'dailyRevenue', 'dailyFees']
    chain_df = chain_df[cols]
    chain_df.insert(4, "CHAIN_NAME", chain_name)
    chain_df.insert(7, "DATE", todays_date)
    return chain_df

# This function executes the set of functions created above 
def extract_and_transform(api_url, chain_name, params):
    chain_df = extract_from_api(api_url, chain_name, params)
    chain_df = transform_data(chain_df, chain_name)
    return chain_df

# Running the ETL functions created on each API link provided
eth_df = extract_and_transform('https://api.llama.fi/overview/fees/ethereum?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'ETHEREUM', params)
arb_df = extract_and_transform('https://api.llama.fi/overview/fees/arbitrum?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'ARBITRUM', params)
op_df = extract_and_transform('https://api.llama.fi/overview/fees/optimism?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'OPTIMISM', params)
bsc_df = extract_and_transform('https://api.llama.fi/overview/fees/BSC?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'BSC', params)
polygon_df = extract_and_transform('https://api.llama.fi/overview/fees/polygon?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'POLYGON', params)
avalanche_df = extract_and_transform('https://api.llama.fi/overview/fees/avalanche?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'AVALANCHE', params)
base_df = extract_and_transform('https://api.llama.fi/overview/fees/base?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'BASE', params)
solana_df = extract_and_transform('https://api.llama.fi/overview/fees/solana?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'SOLANA', params)
cronos_df = extract_and_transform('https://api.llama.fi/overview/fees/cronos?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue', 'CRONOS', params)

# Putting all pandas dataframes into a list
chain_df_list = [eth_df, arb_df, op_df, bsc_df, polygon_df, base_df, avalanche_df, solana_df, cronos_df]

# Connecting with SQL Server database using SQLAlchemy 
SERVER = os.environ.get('MS SQL SERVER NAME')
DRIVER = os.environ.get('MS SQL SERVER DRIVER')
database_name = 'defi_db'
SQL_SERVER_CONNECTION = sqlalchemy.create_engine(f'mssql://{SERVER}/{database_name}?driver={DRIVER}')

cnxn_str = (f"Driver={DRIVER};Server={SERVER};Database=defi_db;Trusted_Connection=yes;")
cnxn = pyodbc.connect(cnxn_str)
cursor = cnxn.cursor()

# This function loads the data extracted from the API into a SQL Server database
def concatenate_and_load(chain_list):
    final_df = pd.concat(chain_list)
    final_df.to_sql('staging_defi_revenue_details', SQL_SERVER_CONNECTION, if_exists='replace', index=False,
                    dtype={'defillamaId': sqlalchemy.types.INTEGER(),
                           'name': sqlalchemy.types.String(50),
                           'module': sqlalchemy.types.String(50),
                           'category': sqlalchemy.types.String(50),
                           'CHAIN_NAME': sqlalchemy.types.String(50),
                           'dailyRevenue': sqlalchemy.types.Numeric(10,2),
                           'dailyFees': sqlalchemy.types.Numeric(10,2),
                           'DATE': sqlalchemy.types.Date()})

-- Removing duplicates from the staging table before inserting into data warehouse table
cursor.execute('WITH check_for_duplicate_data AS (\
                SELECT defillamaId, name, module, category, CHAIN_NAME, dailyRevenue, dailyFees, DATE,\
                ROW_NUMBER() OVER (PARTITION BY defillamaId, name, module, category, CHAIN_NAME, dailyRevenue, dailyFees, DATE\
                ORDER BY defillamaId, name, module, category, CHAIN_NAME, dailyRevenue, dailyFees, DATE) AS duplicate_count\
                FROM [dbo].[staging_defi_revenue_details])\
                DELETE FROM check_for_duplicate_data WHERE duplicate_count > 1')

-- Inserting into data warehouse table
cursor.execute('INSERT INTO dbo.DW_DEFI_REVENUE_TABLE (Defi_lama_ID, Dapp_name, Module, Category, Chain_name, Daily_Revenue, Daily_Fees, Date)\
                SELECT * FROM dbo.staging_defi_revenue_details')

-- Committing the execution 
cnxn.commit()
print('Data successfully loaded into SQL Server database')

-- Closing and disposing connection to the database
SQL_SERVER_CONNECTION.dispose()
cnxn.close()
'''

